# Projets d'Apprentissage par Renforcement

<div align="center">
  <a href="https://github.com/ennajari/reinforcement-learning">
    <img src="https://img.shields.io/badge/GitHub-View_on_GitHub-blue?logo=github" alt="GitHub">
  </a>
</div>

## Table des MatiÃ¨res

<div class="toc">
  <ul>
    <li><a href="#tp1">TP1: DÃ©couverte de Gymnasium</a></li>
    <li><a href="#tp2">TP2: Q-Learning</a></li>
    <li><a href="#tp3">TP3: Gestion de Trafic</a></li>
    <li><a href="#results">RÃ©sultats ClÃ©s</a></li>
    <li><a href="#install">Installation</a></li>
    <li><a href="#workflows">Workflows</a></li>
  </ul>
</div>

<h2 id="tp1">TP1: DÃ©couverte de Gymnasium et CartPole</h2>

### ğŸ¯ Objectifs
<div class="objectives">
  <ul>
    <li>Prise en main de Gymnasium</li>
    <li>Exploration de CartPole-v1</li>
    <li>ComprÃ©hension des concepts RL de base</li>
  </ul>
</div>

### ğŸ“ Code ClÃ©
### CrÃ©ation de l'environnement
    import gymnasium as gym
    env = gym.make("CartPole-v1", render_mode="human")
    observation, info = env.reset()

### Boucle d'interaction
    for _ in range(100):
        action = env.action_space.sample()  # Action alÃ©atoire
        observation, reward, terminated, truncated, info = env.step(action)
        
        if terminated or truncated:
            observation, info = env.reset()
        
### ğŸ”„ Workflow CartPole

```mermaid
flowchart TD
    A["[DÃ©but]"] --> B["[CrÃ©er environnement]"]
    B --> C["[RÃ©initialiser environnement]"]
    C --> D["[Choisir action]"]
    D --> E["[ExÃ©cuter action]"]
    E --> F{"[TerminÃ© ?]"}
    F -->|Non| D
    F -->|Oui| C
```
### ğŸ“ˆ Tableau Comparatif
<table>
  <tr>
    <th>Algorithme</th>
    <th>RÃ©compense Moyenne</th>
    <th>StabilitÃ©</th>
    <th>Temps d'entraÃ®nement</th>
  </tr>
  <tr>
    <td>Q-Learning</td>
    <td>42.7 Â± 3.2</td>
    <td>â­â­â­â­</td>
    <td>3 min</td>
  </tr>
  <tr>
    <td>SARSA</td>
    <td>39.1 Â± 2.8</td>
    <td>â­â­â­â­â­</td>
    <td>5 min</td>
  </tr>
</table>

<h2 id="install">Installation</h2>

# Cloner le dÃ©pÃ´t
    git clone https://github.com/ennajari/reinforcement-learning.git
    cd reinforcement-learning

# Installer les dÃ©pendances
    pip install -r requirements.txt

# ExÃ©cuter les TPs
     tp1.ipynb  # CartPole
     tp2.ipynb  # FrozenLake
     tp3.ipynb  # Traffic Management

<h2 id="results">RÃ©sultats ClÃ©s</h2>

<div class="highlight">
  <h3>Principales Conclusions</h3>
  <ul>
    <li>ğŸ” <strong>Q-Learning</strong> converge 25% plus vite que SARSA</li>
    <li>ğŸ›¡ï¸ <strong>SARSA</strong> montre une meilleure stabilitÃ© (Ã©cart-type rÃ©duit de 15%)</li>
    <li>ğŸš¦ Meilleure politique: Q-Learning pour les performances, SARSA pour la sÃ©curitÃ©</li>
  </ul>
</div>

<h2 id="workflows">Workflows Complets</h2>

### Workflow Global du Projet
```mermaid
flowchart TB
    subgraph TP1
    A1[Environnement] --> B1[Exploration]
    B1 --> C1[ContrÃ´le manuel]
    end
    
    subgraph TP2
    A2[Q-Table] --> B2[Apprentissage]
    B2 --> C2[Ã‰valuation]
    end
    
    subgraph TP3
    A3[Environnement Traffic] --> B3[Q-Learning vs SARSA]
    B3 --> C3[Analyse comparative]
    end
    
    TP1 --> TP2 --> TP3
```
