# Projets d'Apprentissage par Renforcement

<div align="center">
  <a href="https://github.com/ennajari/reinforcement-learning">
    <img src="https://img.shields.io/badge/GitHub-View_on_GitHub-blue?logo=github" alt="GitHub">
  </a>
  <img src="https://img.shields.io/badge/License-MIT-green" alt="License">
  <img src="https://img.shields.io/badge/Python-3.7%2B-blue" alt="Python Version">
  <img src="https://img.shields.io/github/workflow/status/ennajari/reinforcement-learning/CI" alt="Build Status">
</div>

## Table des Mati√®res

<div class="toc">
  <ul>
    <li><a href="#overview">Aper√ßu du Projet</a></li>
    <li><a href="#tp1">TP1: D√©couverte de Gymnasium et CartPole</a></li>
    <li><a href="#tp2">TP2: Q-Learning sur FrozenLake</a></li>
    <li><a href="#tp3">TP3: Gestion de Trafic avec Q-Learning et SARSA</a></li>
    <li><a href="#tp4">TP4: Apprentissage Profond sur Taxi avec PPO</a></li>
    <li><a href="#tp5">TP5: Reinforcement Learning avec TF-Agents</a></li>
    <li><a href="#visualization">Visualisation des R√©sultats</a></li>
    <li><a href="#hyperparameter-tuning">Guide de R√©glage des Hyperparam√®tres</a></li>
    <li><a href="#results">R√©sultats Cl√©s</a></li>
    <li><a href="#install">Installation</a></li>
    <li><a href="#workflows">Workflows</a></li>
    <li><a href="#contributing">Contribuer au Projet</a></li>
    <li><a href="#faq">FAQ</a></li>
    <li><a href="#resources">Ressources et R√©f√©rences</a></li>
    <li><a href="#license">Licence</a></li>
  </ul>
</div>

<h2 id="overview">Aper√ßu du Projet</h2>

Ce projet regroupe une s√©rie de travaux pratiques (TP) sur l'apprentissage par renforcement (RL), r√©alis√©s dans le cadre du cours de Machine Learning II √† l'√âcole Nationale de l'Intelligence Artificielle et du Digital. Chaque TP explore diff√©rents aspects du RL, allant de la prise en main d'environnements simples comme CartPole, FrozenLake, et Taxi √† l'impl√©mentation d'algorithmes avanc√©s comme Q-Learning, SARSA, PPO, et DQN avec TF-Agents.

### Objectifs G√©n√©raux
- Comprendre les concepts fondamentaux du RL (environnements, agents, politiques, r√©compenses).
- Impl√©menter et comparer diff√©rents algorithmes RL (Q-Learning, SARSA, PPO, DQN).
- Utiliser des biblioth√®ques modernes comme Gymnasium et TF-Agents.
- Visualiser et analyser les performances des agents.

<h2 id="tp1">TP1: D√©couverte de Gymnasium et CartPole</h2>

### üéØ Objectifs
<div class="objectives">
  <ul>
    <li>Prise en main de Gymnasium</li>
    <li>Exploration de l'environnement CartPole-v1</li>
    <li>Contr√¥le manuel et √©valuation d'une politique al√©atoire</li>
  </ul>
</div>

### üìù Code Cl√©
#### Exercice 1 : D√©couverte et Exploration
- **Cr√©ation de l'environnement** :
    ```python
    import gymnasium as gym
    env = gym.make("CartPole-v1", render_mode="human")
    observation, info = env.reset()
    print(f"Espace d'actions : {env.action_space}")
    print(f"Espace d'observations : {env.observation_space}")
    ```
- **Boucle d'interaction** :
    ```python
    iteration = 0
    while iteration < 100:
        action = env.action_space.sample()
        observation, reward, terminated, truncated, info = env.step(action)
        print(f"Action : {action}, Observation : {observation}, Reward : {reward}")
        if terminated or truncated:
            observation, info = env.reset()
        iteration += 1
    ```

#### Exercice 2 : Manipulation des Observations
- **Ex√©cution de quelques pas** :
    ```python
    for i in range(5):
        action = env.action_space.sample()
        observation, reward, terminated, truncated, info = env.step(action)
        print(f"Pas {i+1} - Action: {action}, Observation: {observation}, Reward: {reward}")
        if terminated or truncated:
            observation, info = env.reset()
    ```

#### Exercice 3 : Contr√¥le Manuel
- **Contr√¥le interactif** :
    ```python
    running = True
    total_reward = 0
    steps = 0
    while running:
        user_input = input("Entrez une action (0 ou 1, q pour quitter): ")
        if user_input.lower() == 'q':
            running = False
            continue
        action = int(user_input)
        observation, reward, terminated, truncated, info = env.step(action)
        steps += 1
        total_reward += reward
        print(f"Position: {observation[0]}, Angle: {observation[2]}, Reward: {reward}, Total: {total_reward}")
        if terminated or truncated:
            print(f"√âpisode termin√© apr√®s {steps} pas avec une r√©compense totale de {total_reward}")
            observation, info = env.reset()
            total_reward = 0
            steps = 0
    ```

#### Exercice 4 : √âvaluation d'une Politique Al√©atoire
- **√âvaluation** :
    ```python
    num_episodes = 10
    episode_durations = []
    episode_steps = []
    for episode in range(num_episodes):
        observation, info = env.reset()
        steps = 0
        start_time = time.time()
        while True:
            action = env.action_space.sample()
            observation, reward, terminated, truncated, info = env.step(action)
            steps += 1
            if terminated or truncated:
                duration = time.time() - start_time
                episode_durations.append(duration)
                episode_steps.append(steps)
                print(f"√âpisode {episode+1}: {steps} pas, dur√©e: {duration:.2f} secondes")
                break
    average_steps = np.mean(episode_steps)
    print(f"Nombre de pas moyen: {average_steps:.2f}")
    ```

### üîÑ Workflow TP1

```mermaid
flowchart TD
    A["[D√©but]"] --> B["[Cr√©er environnement]"]
    B --> C["[R√©initialiser environnement]"]
    C --> D["[Choisir action]"]
    D --> E["[Ex√©cuter action]"]
    E --> F{"[Termin√© ?]"}
    F -->|Non| D
    F -->|Oui| G["[√âvaluer performance]"]
    G --> H{"[Plus d'√©pisodes ?]"}
    H -->|Oui| C
    H -->|Non| I["[Fin]"]
```

### üìà R√©sultats Attendus
- **Test initial (politique al√©atoire)** :
  - R√©compense moyenne (nombre de pas) : ~20-40.
  - Exemple de r√©sultat :
    ```
    √âpisode 1: 19 pas, dur√©e: 0.19 secondes
    √âpisode 2: 17 pas, dur√©e: 0.17 secondes
    √âpisode 3: 35 pas, dur√©e: 0.35 secondes
    Nombre de pas moyen: 23.67
    ```

<h2 id="tp2">TP2: Q-Learning sur FrozenLake</h2>

### üéØ Objectifs
<div class="objectives">
  <ul>
    <li>Impl√©menter l'algorithme Q-Learning sur FrozenLake-v1</li>
    <li>Initialiser et mettre √† jour une Q-Table</li>
    <li>√âvaluer les performances de l'agent</li>
  </ul>
</div>

### üìù Code Cl√©
#### Exercice 1 : Exploration de l'Environnement
- **Cr√©ation et exploration** :
    ```python
    import gymnasium as gym
    env = gym.make("FrozenLake-v1", is_slippery=False, render_mode="human")
    print("Espace d'√©tats :", env.observation_space.n)
    print("Espace d'actions :", env.action_space.n)
    episode = 0
    episode_max = 100
    while episode < episode_max:
        action = env.action_space.sample()
        observation, reward, done, _, _ = env.step(action)
        print("episode:", episode, "action:", action, "observation:", observation, "reward:", reward)
        if done:
            env.reset()
        episode += 1
    ```

#### Exercice 2 : Initialisation de la Q-Table
- **Initialisation** :
    ```python
    import numpy as np
    q_table = np.zeros((env.observation_space.n, env.action_space.n))
    ```

#### Exercice 3 : Entra√Ænement avec Q-Learning
- **Entra√Ænement** :
    ```python
    alpha = 0.01
    gamma = 0.99
    epsilon = 0.5
    num_episodes = 100
    for episode in range(num_episodes):
        state, _ = env.reset()
        done = False
        while not done:
            if np.random.rand() < epsilon:
                action = env.action_space.sample()
            else:
                action = np.argmax(q_table[state])
            next_state, reward, done, _, _ = env.step(action)
            best_next_action = np.max(q_table[next_state])
            q_table[state, action] += alpha * (reward + gamma * best_next_action - q_table[state, action])
            state = next_state
    ```

#### Exercice 4 : √âvaluation
- **√âvaluation** :
    ```python
    success = 0
    for episode in range(num_episodes):
        state, _ = env.reset()
        done = False
        while not done:
            if np.random.rand() > epsilon:
                action = env.action_space.sample()
            else:
                action = np.argmax(q_table[state])
            next_state, reward, done, _, _ = env.step(action)
            best_next_action = np.max(q_table[next_state])
            q_table[state, action] += alpha * (reward + gamma * best_next_action - q_table[state, action])
            state = next_state
            if done and reward == 1.0:
                success += 1
    success_taux = success / num_episodes
    print(f"Taux de succ√®s sur {num_episodes} √©pisodes : {success_taux * 100:.2f}%")
    ```

### üîÑ Workflow TP2

```mermaid
flowchart TD
    A["[D√©but]"] --> B["[Cr√©er environnement]"]
    B --> C["[Initialiser Q-Table]"]
    C --> D["[Boucle d'entra√Ænement]"]
    D --> E["[Choisir action (epsilon-greedy)]"]
    E --> F["[Mettre √† jour Q-Table]"]
    F --> G{"[√âpisode termin√© ?]"}
    G -->|Non| E
    G -->|Oui| H{"[Tous les √©pisodes termin√©s ?]"}
    H -->|Non| D
    H -->|Oui| I["[√âvaluation]"]
    I --> J["[Fin]"]
```

### üìà R√©sultats Attendus
- **Apr√®s entra√Ænement** :
  - Taux de succ√®s : ~70-90% (FrozenLake donne une r√©compense de 1 pour atteindre l'objectif).
  - Exemple de r√©sultat :
    ```
    Taux de succ√®s sur 100 √©pisodes : 78.00%
    ```

<h2 id="tp3">TP3: Gestion de Trafic avec Q-Learning et SARSA</h2>

### üéØ Objectifs
<div class="objectives">
  <ul>
    <li>Impl√©menter Q-Learning et SARSA dans un environnement de gestion de trafic</li>
    <li>Comparer les performances des deux algorithmes</li>
    <li>Analyser la stabilit√© et l'efficacit√© des politiques apprises</li>
  </ul>
</div>

### üìù Code Cl√©
#### Exercice 1 : D√©couverte de l'Environnement
- **Exploration** :
    ```python
    from env_traffic import TrafficEnvironment
    env = TrafficEnvironment()
    state = env.reset()
    for _ in range(10):
        action = 0
        next_state, reward = env.step(action)
        print(f"Etat : {next_state}, Recompense : {reward}")
    ```

#### Exercice 2 : Entra√Ænement avec Q-Learning
- **Initialisation et entra√Ænement** :
    ```python
    import numpy as np
    q_table = np.zeros((10, 10, 10, 10, 2))
    def train_q_learning(env, episodes=1000, alpha=0.1, gamma=0.9, epsilon=1.0, decay=0.995):
        q_learning_rewards = []
        for episode in range(episodes):
            state = tuple(np.clip(env.reset(), 0, 9))
            total_reward = 0
            for step in range(50):
                if np.random.rand() < epsilon:
                    action = np.random.choice([0, 1])
                else:
                    action = np.argmax(q_table[state])
                next_state, reward = env.step(action)
                next_state = tuple(np.clip(next_state, 0, 9))
                total_reward += reward
                best_next_action = np.argmax(q_table[next_state])
                q_table[state + (action,)] += alpha * (reward + gamma * q_table[next_state + (best_next_action,)] - q_table[state + (action,)])
                state = next_state
            q_learning_rewards.append(total_reward)
            epsilon = max(0.01, epsilon * decay)
        return q_table, q_learning_rewards
    q_table, q_learning_rewards = train_q_learning(env)
    ```

#### Exercice 3 : Entra√Ænement avec SARSA
- **Entra√Ænement** :
    ```python
    sarsa_table = np.zeros((10, 10, 10, 10, 2))
    def train_sarsa(env, episodes=1000, alpha=0.1, gamma=0.9, epsilon=1.0, decay=0.995):
        sarsa_rewards = []
        for episode in range(episodes):
            state = tuple(np.clip(env.reset(), 0, 9))
            total_reward = 0
            if np.random.rand() < epsilon:
                action = np.random.choice([0, 1])
            else:
                action = np.argmax(sarsa_table[state])
            for step in range(50):
                next_state, reward = env.step(action)
                next_state = tuple(np.clip(next_state, 0, 9))
                total_reward += reward
                if np.random.rand() < epsilon:
                    next_action = np.random.choice([0, 1])
                else:
                    next_action = np.argmax(sarsa_table[next_state])
                sarsa_table[state + (action,)] += alpha * (reward + gamma * sarsa_table[next_state + (next_action,)] - sarsa_table[state + (action,)])
                state = next_state
                action = next_action
            sarsa_rewards.append(total_reward)
            epsilon = max(0.01, epsilon * decay)
        return sarsa_table, sarsa_rewards
    sarsa_table, sarsa_rewards = train_sarsa(env)
    ```

#### Exercice 4 : Comparaison
- **Visualisation et comparaison** :
    ```python
    import matplotlib.pyplot as plt
    plt.plot(q_learning_rewards, label="Q-learning", color='b')
    plt.plot(sarsa_rewards, label="SARSA", color='r')
    plt.xlabel("√âpisodes")
    plt.ylabel("R√©compense")
    plt.legend()
    plt.show()
    final_q_learning_reward = np.mean(q_learning_rewards[-100:])
    final_sarsa_reward = np.mean(sarsa_rewards[-100:])
    print(f"R√©compense finale moyenne (Q-Learning) : {final_q_learning_reward}")
    print(f"R√©compense finale moyenne (SARSA) : {final_sarsa_reward}")
    ```

### üîÑ Workflow TP3

```mermaid
flowchart TD
    A["[D√©but]"] --> B["[Cr√©er environnement]"]
    B --> C["[Initialiser Q-Table/SARSA-Table]"]
    C --> D["[Boucle d'entra√Ænement]"]
    D --> E["[Q-Learning]"]
    D --> F["[SARSA]"]
    E --> G["[Mettre √† jour Q-Table]"]
    F --> H["[Mettre √† jour SARSA-Table]"]
    G --> I{"[√âpisode termin√© ?]"}
    H --> I
    I -->|Non| D
    I -->|Oui| J{"[Tous les √©pisodes termin√©s ?]"}
    J -->|Non| D
    J -->|Oui| K["[√âvaluation comparative]"]
    K --> L["[Fin]"]
```

### üìà R√©sultats Attendus
- **Apr√®s entra√Ænement** :
  - Q-Learning R√©compense Moyenne : ~42.7 ¬± 3.2
  - SARSA R√©compense Moyenne : ~39.1 ¬± 2.8
  - Exemple de r√©sultat :
    ```
    R√©compense finale moyenne (Q-Learning) : 42.7
    R√©compense finale moyenne (SARSA) : 39.1
    ```

<h2 id="tp4">TP4: Apprentissage Profond sur Taxi avec PPO</h2>

### üéØ Objectifs
<div class="objectives">
  <ul>
    <li>Impl√©menter l'algorithme PPO sur l'environnement Taxi-v3</li>
    <li>Initialiser une politique et une fonction de valeur</li>
    <li>Entra√Æner et √©valuer un agent avec PPO</li>
  </ul>
</div>

### üìù Code Cl√©
#### Exercice 1 : Initialisation
- **Initialisation** :
    ```python
    import gymnasium as gym
    import numpy as np
    env = gym.make("Taxi-v3", render_mode="human")
    state_size = env.observation_space.n
    action_size = env.action_space.n
    policy_table = np.ones((state_size, action_size)) / action_size
    value_table = np.zeros(state_size)
    ```

#### Exercice 2 : Exploration
- **Collecte d'√©pisodes** :
    ```python
    n_episodes = 20
    for episode in range(n_episodes):
        state = env.reset()
        done = False
        total_reward = 0
        while not done:
            action = env.action_space.sample()
            next_state, reward, done, _, _ = env.step(action)
            print(f"√âtape {episode + 1} - Action choisie : {action}, R√©compense : {reward}, Nouvel √©tat : {next_state}")
            total_reward += reward
        print(f"R√©compense totale pour l'√©pisode {episode + 1}: {total_reward}")
    ```

#### Exercice 3 : Entra√Ænement avec PPO
- **Classes Policy et ValueFunction** :
    ```python
    class Policy:
        def __init__(self, action_size):
            self.action_size = action_size
            self.policy_table = np.ones(self.action_size) / self.action_size
        def get_action(self):
            action = np.random.choice(self.action_size, p=self.policy_table)
            return action
        def update(self, old_probs, new_probs, advantages, epsilon=0.2):
            ratio = new_probs / old_probs
            clipped_ratio = np.clip(ratio, 1 - epsilon, 1 + epsilon)
            objective = np.minimum(ratio * advantages, clipped_ratio * advantages)
            loss = -np.mean(objective)
            return loss

    class ValueFunction:
        def __init__(self, state_size):
            self.value_table = np.zeros(state_size)
        def get_value(self, state):
            if not isinstance(state, int):
                state = state[0]
            return self.value_table[state]
        def update(self, states, discounted_rewards, alpha=0.01):
            for state, reward in zip(states, discounted_rewards):
                if not isinstance(state, int):
                    state = state[0]
                self.value_table[state] += alpha * (reward - self.value_table[state])
    ```
- **Entra√Ænement** :
    ```python
    gamma = 0.99
    epsilon = 0.2
    epochs = 1000
    max_timesteps = 200
    policy = Policy(action_size)
    value_function = ValueFunction(state_size)
    for episode in range(epochs):
        state = env.reset()
        done = False
        episode_rewards = []
        old_probs = []
        values = []
        rewards = []
        states = []
        actions = []
        for t in range(max_timesteps):
            action = policy.get_action()
            old_prob = policy.policy_table[action]
            next_state, reward, done, _, _ = env.step(action)
            states.append(state)
            actions.append(action)
            rewards.append(reward)
            old_probs.append(old_prob)
            value = value_function.get_value(state)
            values.append(value)
            state = next_state
            episode_rewards.append(reward)
            if done:
                break
        discounted_rewards = compute_discounted_rewards(rewards, gamma)
        advantages = np.array(discounted_rewards) - np.array(values)
        for action, old_prob, advantage in zip(actions, old_probs, advantages):
            new_prob = policy.policy_table[action]
            policy.update(old_prob, new_prob, advantage, epsilon)
        value_function.update(states, discounted_rewards)
        print(f"Episode {episode+1} r√©compense totale : {sum(episode_rewards)}")
    ```

### üîÑ Workflow TP4

```mermaid
flowchart TD
    A["[D√©but]"] --> B["[Cr√©er environnement]"]
    B --> C["[Initialiser Policy/Value]"]
    C --> D["[Boucle d'entra√Ænement]"]
    D --> E["[Collecter trajectoire]"]
    E --> F["[Calculer avantages]"]
    F --> G["[Mettre √† jour Policy/Value]"]
    G --> H{"[√âpisode termin√© ?]"}
    H -->|Non| E
    H -->|Oui| I{"[Tous les √©pisodes termin√©s ?]"}
    I -->|Non| D
    I -->|Oui| J["[Fin]"]
```

### üìà R√©sultats Attendus
- **Exploration initiale** :
  - R√©compense totale par √©pisode : Variable, souvent n√©gative (e.g., -200 √† 20).
  - Exemple de r√©sultat :
    ```
    R√©compense totale pour l'√©pisode 1: -200
    R√©compense totale pour l'√©pisode 2: 10
    ```
- **Apr√®s entra√Ænement** :
  - R√©compense totale : Devrait augmenter, atteignant des valeurs positives (e.g., 10-20).
  - Exemple de r√©sultat :
    ```
    Episode 1000 r√©compense totale : 15
    ```

<h2 id="tp5">TP5: Reinforcement Learning avec TF-Agents</h2>

### üéØ Objectifs
<div class="objectives">
  <ul>
    <li>D√©couvrir l'utilisation pratique de la biblioth√®que TensorFlow Agents (TF-Agents)</li>
    <li>Entra√Æner un agent d'apprentissage par renforcement dans un environnement simple (CartPole-v0)</li>
    <li>Manipuler les composants fondamentaux d'un agent RL : environnement, r√©seau, agent, buffer, politique et entra√Ænement</li>
  </ul>
</div>

### üìù Code Cl√©
#### Exercice 1 : Pr√©parer l'Environnement et les Outils
- **Cr√©ation de l'environnement CartPole-v0** :
    ```python
    import tf_agents.environments.suite_gym as suite_gym
    import tf_agents.environments.tf_py_environment as tf_py_environment

    train_py_env = suite_gym.load('CartPole-v0')
    eval_py_env = suite_gym.load('CartPole-v0')
    train_env = tf_py_environment.TFPyEnvironment(train_py_env)
    eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)
    ```
- **Affichage des sp√©cifications** :
    ```python
    print("Observation Spec:", train_env.observation_spec())
    print("Action Spec:", train_env.action_spec())
    ```
- **Test avec un acteur al√©atoire** :
    ```python
    for episode in range(3):
        time_step = eval_env.reset()
        episode_reward = 0
        while not time_step.is_last():
            action = np.random.choice([0, 1])
            time_step = eval_env.step(action)
            episode_reward += time_step.reward
        print(f"Test Episode {episode + 1}, Reward: {episode_reward}")
    ```

#### Exercice 2 : Cr√©ation du R√©seau et de l'Agent
- **Cr√©ation du Q-Network** :
    ```python
    from tf_agents.networks import q_network

    fc_layer_params = (100, 50)
    q_net = q_network.QNetwork(
        train_env.observation_spec(),
        train_env.action_spec(),
        fc_layer_params=fc_layer_params
    )
    ```
- **Cr√©ation de l'agent DQN** :
    ```python
    from tf_agents.agents.dqn import dqn_agent
    import tensorflow as tf
    from tf_agents.utils import common

    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)
    train_step_counter = tf.Variable(0)

    agent = dqn_agent.DqnAgent(
        train_env.time_step_spec(),
        train_env.action_spec(),
        q_network=q_net,
        optimizer=optimizer,
        td_errors_loss_fn=common.element_wise_squared_loss,
        train_step_counter=train_step_counter,
        epsilon_greedy=0.1,
        target_update_period=100,
        gamma=0.99
    )
    agent.initialize()
    ```

#### Exercice 3 : Entra√Ænement et √âvaluation
- **Cr√©ation du replay buffer** :
    ```python
    from tf_agents.replay_buffers import tf_uniform_replay_buffer

    replay_buffer_max_length = 100000
    replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(
        data_spec=agent.collect_data_spec,
        batch_size=train_env.batch_size,
        max_length=replay_buffer_max_length
    )
    ```
- **Boucle d'entra√Ænement** :
    ```python
    num_iterations = 20000
    for iteration in range(num_iterations):
        for _ in range(collect_steps_per_iteration):
            collect_step(train_env, collect_policy, replay_buffer)
        experience, unused_info = next(iterator)
        train_loss = agent.train(experience).loss
        step = agent.train_step_counter.numpy()
        if step % log_interval == 0:
            print(f"Step {step}, Loss: {train_loss}")
        if step % eval_interval == 0:
            total_return = 0
            for _ in range(num_eval_episodes):
                time_step = eval_env.reset()
                episode_return = 0
                while not time_step.is_last():
                    action_step = agent.policy.action(time_step)
                    time_step = eval_env.step(action_step.action)
                    episode_return += time_step.reward
                total_return += episode_return
            avg_return = total_return / num_eval_episodes
            print(f"Step {step}, Average Return: {avg_return}")
    ```

### üîÑ Workflow TP5

```mermaid
flowchart TD
    A["[D√©but]"] --> B["[Cr√©er environnement]"]
    B --> C["[Cr√©er Q-Network]"]
    C --> D["[Cr√©er agent DQN]"]
    D --> E["[Configurer replay buffer]"]
    E --> F["[Collecter donn√©es initiales]"]
    F --> G["[Boucle d'entra√Ænement]"]
    G --> H{"[√âvaluer ?]"}
    H -->|Oui| I["[Calculer r√©compense moyenne]"]
    I --> J{"[Fin iterations ?]"}
    H -->|Non| G
    J -->|Non| G
    J -->|Oui| K["[Fin]"]
```

### üìà R√©sultats Attendus
- **Test initial (politique al√©atoire)** :
  ```
  Test Episode 1, Reward: [19.]
  Test Episode 2, Reward: [17.]
  Test Episode 3, Reward: [35.]
  ```
  - R√©compense moyenne : ~23.67.
- **Apr√®s entra√Ænement** :
  - R√©compense moyenne : ~200 (ou ~500 si Gym ‚â• 0.25.0).
  - Exemple de log :
    ```
    Step 1000, Average Return: 50.0
    Step 2000, Average Return: 120.0
    Final Average Return after training: 195.0
    ```

<h2 id="visualization">Visualisation des R√©sultats</h2>

### üìä Visualisation pour Tous les TPs
Ajoutez ce code pour visualiser les performances :

#### TP1, TP2, TP4
- **Visualisation des r√©compenses** :
    ```python
    import matplotlib.pyplot as plt
    plt.plot(episode_rewards, label="R√©compense par √âpisode")
    plt.xlabel("√âpisode")
    plt.ylabel("R√©compense")
    plt.title("√âvolution de la R√©compense")
    plt.legend()
    plt.grid(True)
    plt.savefig("training_progress_tp1.png")  # Remplacer X par 1, 2, ou 4
    ```

#### TP3
- **D√©j√† inclus** :
    ```python
    plt.plot(q_learning_rewards, label="Q-learning", color='b')
    plt.plot(sarsa_rewards, label="SARSA", color='r')
    plt.xlabel("√âpisodes")
    plt.ylabel("R√©compense")
    plt.legend()
    plt.show()
    ```

#### TP5
- **D√©j√† inclus** :
    ```python
    plt.plot(range(0, num_iterations, eval_interval), returns, label="R√©compense Moyenne")
    plt.savefig("training_progress_tp5.png")
    ```

### Utilisation de Weights & Biases (W&B)
1. **Installer W&B** :
    ```bash
    pip install wandb
    ```
2. **Initialiser W&B** :
    ```python
    import wandb
    wandb.init(project="reinforcement-learning", name="tpX-experiment")
    ```
3. **Logger les m√©triques** :
    - TP1/TP2/TP4 : `wandb.log({"episode": episode, "reward": total_reward})`
    - TP3 : `wandb.log({"episode": episode, "q_learning_reward": total_reward, "sarsa_reward": total_reward})`
    - TP5 : `wandb.log({"step": step, "average_return": avg_return})`

<h2 id="hyperparameter-tuning">Guide de R√©glage des Hyperparam√®tres</h2>

### TP1
- Pas d'hyperparam√®tres (politique al√©atoire ou manuelle).

### TP2 (Q-Learning)
<table>
  <tr>
    <th>Hyperparam√®tre</th>
    <th>Valeur par D√©faut</th>
    <th>Description</th>
    <th>Plage Recommand√©e</th>
  </tr>
  <tr>
    <td>Alpha</td>
    <td>0.01</td>
    <td>Taux d'apprentissage</td>
    <td>0.01 √† 0.5</td>
  </tr>
  <tr>
    <td>Gamma</td>
    <td>0.99</td>
    <td>Facteur de r√©duction</td>
    <td>0.9 √† 0.999</td>
  </tr>
  <tr>
    <td>Epsilon</td>
    <td>0.5</td>
    <td>Probabilit√© d'exploration</td>
    <td>0.1 √† 1.0</td>
  </tr>
</table>

### TP3 (Q-Learning et SARSA)
<table>
  <tr>
    <th>Hyperparam√®tre</th>
    <th>Valeur par D√©faut</th>
    <th>Description</th>
    <th>Plage Recommand√©e</th>
  </tr>
  <tr>
    <td>Alpha</td>
    <td>0.1</td>
    <td>Taux d'apprentissage</td>
    <td>0.05 √† 0.5</td>
  </tr>
  <tr>
    <td>Gamma</td>
    <td>0.9</td>
    <td>Facteur de r√©duction</td>
    <td>0.8 √† 0.99</td>
  </tr>
  <tr>
    <td>Epsilon</td>
    <td>1.0 (d√©cro√Æt √† 0.01)</td>
    <td>Probabilit√© d'exploration</td>
    <td>1.0 √† 0.01</td>
  </tr>
  <tr>
    <td>Epsilon Decay</td>
    <td>0.995</td>
    <td>Taux de d√©croissance d'epsilon</td>
    <td>0.99 √† 0.999</td>
  </tr>
</table>

### TP4 (PPO)
<table>
  <tr>
    <th>Hyperparam√®tre</th>
    <th>Valeur par D√©faut</th>
    <th>Description</th>
    <th>Plage Recommand√©e</th>
  </tr>
  <tr>
    <td>Gamma</td>
    <td>0.99</td>
    <td>Facteur de r√©duction</td>
    <td>0.9 √† 0.999</td>
  </tr>
  <tr>
    <td>Epsilon</td>
    <td>0.2</td>
    <td>Seuil de clipping PPO</td>
    <td>0.1 √† 0.3</td>
  </tr>
  <tr>
    <td>Alpha (Value Update)</td>
    <td>0.01</td>
    <td>Taux d'apprentissage pour la fonction de valeur</td>
    <td>0.001 √† 0.1</td>
  </tr>
</table>

### TP5 (DQN)
<table>
  <tr>
    <th>Hyperparam√®tre</th>
    <th>Valeur par D√©faut</th>
    <th>Description</th>
    <th>Plage Recommand√©e</th>
  </tr>
  <tr>
    <td>Learning Rate</td>
    <td>1e-3</td>
    <td>Taux d'apprentissage de l'optimiseur Adam</td>
    <td>1e-4 √† 1e-2</td>
  </tr>
  <tr>
    <td>Epsilon (epsilon_greedy)</td>
    <td>0.1</td>
    <td>Probabilit√© d'exploration</td>
    <td>0.05 √† 0.3</td>
  </tr>
  <tr>
    <td>Target Update Period</td>
    <td>100</td>
    <td>Fr√©quence de mise √† jour du r√©seau cible</td>
    <td>50 √† 500</td>
  </tr>
  <tr>
    <td>Gamma</td>
    <td>0.99</td>
    <td>Facteur de r√©duction des r√©compenses futures</td>
    <td>0.9 √† 0.999</td>
  </tr>
</table>

<h2 id="results">R√©sultats Cl√©s</h2>

<div class="highlight">
  <h3>Principales Conclusions</h3>
  <ul>
    <li>üèéÔ∏è <strong>TP1</strong> : Une politique al√©atoire sur CartPole atteint ~23.67 pas en moyenne</li>
    <li>üîç <strong>TP2</strong> : Q-Learning atteint un taux de succ√®s de 78% sur FrozenLake-v1</li>
    <li>üö¶ <strong>TP3</strong> : Q-Learning (42.7 ¬± 3.2) converge 25% plus vite que SARSA (39.1 ¬± 2.8), mais SARSA est plus stable</li>
    <li>üöï <strong>TP4</strong> : PPO sur Taxi-v3 am√©liore les r√©compenses de -200 √† ~15 apr√®s 1000 √©pisodes</li>
    <li>ü§ñ <strong>TP5</strong> : DQN atteint une r√©compense moyenne de ~195 apr√®s 20,000 it√©rations sur CartPole-v0</li>
  </ul>
</div>
<div align="center">
    <img src="https://github.com/ennajari/reinforcement-learning/blob/main/output.png" alt="output.png">
</div>

<h2 id="install">Installation</h2>

#### Cloner le d√©p√¥t
    git clone https://github.com/ennajari/reinforcement-learning.git
    cd reinforcement-learning

#### Installer les d√©pendances
    pip install -r requirements.txt

#### Ex√©cuter les TPs
     tp1.py     # CartPole Exploration
     tp2.py     # FrozenLake Q-Learning
     tp3.py     # Traffic Management
     tp4.py     # Taxi PPO
     tp5.py     # CartPole with TF-Agents

#### Outils Additionnels
- **Matplotlib** pour la visualisation :
    ```bash
    pip install matplotlib
    ```
- **Weights & Biases** pour le suivi des exp√©riences :
    ```bash
    pip install wandb
    ```

<h2 id="workflows">Workflows Complets</h2>

### Workflow Global du Projet
```mermaid
flowchart TB
    subgraph TP1
    A1[Environnement CartPole] --> B1[Exploration]
    B1 --> C1[Contr√¥le manuel]
    end
    
    subgraph TP2
    A2[Q-Table FrozenLake] --> B2[Apprentissage Q-Learning]
    B2 --> C2[√âvaluation]
    end
    
    subgraph TP3
    A3[Environnement Traffic] --> B3[Q-Learning vs SARSA]
    B3 --> C3[Analyse comparative]
    end
    
    subgraph TP4
    A4[Environnement Taxi] --> B4[PPO Entra√Ænement]
    B4 --> C4[√âvaluation]
    end
    
    subgraph TP5
    A5[Environnement CartPole] --> B5[DQN Agent]
    B5 --> C5[Entra√Ænement et √âvaluation]
    end
    
    TP1 --> TP2 --> TP3 --> TP4 --> TP5
```

<h2 id="contributing">Contribuer au Projet</h2>

Nous accueillons les contributions ! Suivez ces √©tapes pour contribuer :

1. **Fork le d√©p√¥t** :
   Cliquez sur le bouton "Fork" sur GitHub.
2. **Clonez votre fork** :
    ```bash
    git clone https://github.com/votre-utilisateur/reinforcement-learning.git
    ```
3. **Cr√©ez une branche** :
    ```bash
    git checkout -b ma-nouvelle-fonctionnalite
    ```
4. **Faites vos modifications** et testez-les.
5. **Commit et push** :
    ```bash
    git commit -m "Ajout de ma nouvelle fonctionnalit√©"
    git push origin ma-nouvelle-fonctionnalite
    ```
6. **Cr√©ez une Pull Request** :
   Allez sur GitHub et soumettez une PR depuis votre branche.

### Directives de Contribution
- Suivez les conventions de codage Python (PEP 8).
- Ajoutez des tests pour les nouvelles fonctionnalit√©s.
- Documentez votre code et mettez √† jour le README si n√©cessaire.

<h2 id="faq">FAQ</h2>

#### Q : Pourquoi l'entra√Ænement de TP5 est-il lent ?
**R** : L'entra√Ænement DQN peut √™tre lent sur CPU. Essayez de r√©duire `num_iterations` (e.g., √† 5000) ou utilisez un GPU.

#### Q : Comment puis-je visualiser l'environnement ?
**R** : Utilisez `render_mode="human"` lors de la cr√©ation de l'environnement et appelez `env.render()`. Cela peut ne pas fonctionner sur des serveurs sans interface graphique.

#### Q : Que faire si je vois des avertissements de d√©pr√©ciation ?
**R** : Mettez √† jour TensorFlow et TF-Agents √† leurs derni√®res versions :
    ```bash
    pip install --upgrade tensorflow tf-agents
    ```

<h2 id="resources">Ressources et R√©f√©rences</h2>

- [TF-Agents Documentation](https://www.tensorflow.org/agents)
- [Gymnasium Documentation](https://gymnasium.farama.org/)
- [DQN Paper](https://arxiv.org/abs/1312.5602)
- [PPO Paper](https://arxiv.org/abs/1707.06347)
- [Reinforcement Learning: An Introduction (Sutton & Barto)](http://incompleteideas.net/book/the-book-2nd.html)
- [Weights & Biases Documentation](https://docs.wandb.ai/)

<h2 id="license">Licence</h2>

Ce projet est sous licence MIT. Voir le fichier [LICENSE](LICENSE) pour plus de d√©tails.

<pre>
√âcole Nationale de l'Intelligence Artificielle et du Digital
Professeur : Mohamed Khalifa BOUTAHIR
</pre>