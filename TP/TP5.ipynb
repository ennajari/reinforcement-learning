{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43a5268f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tf_agents as tfa\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.environments import suite_gym, tf_py_environment\n",
    "from tf_agents.networks import q_network\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.utils import common\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7dfa3dd",
   "metadata": {},
   "source": [
    "# Exercise 1: Prepare the Environment and Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bc6b131",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_py_env = suite_gym.load('CartPole-v0')\n",
    "eval_py_env = suite_gym.load('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0956dbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6bad3b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Spec: BoundedTensorSpec(shape=(4,), dtype=tf.float32, name='observation', minimum=array([-4.8000002e+00, -3.4028235e+38, -4.1887903e-01, -3.4028235e+38],\n",
      "      dtype=float32), maximum=array([4.8000002e+00, 3.4028235e+38, 4.1887903e-01, 3.4028235e+38],\n",
      "      dtype=float32))\n",
      "Action Spec: BoundedTensorSpec(shape=(), dtype=tf.int64, name='action', minimum=array(0, dtype=int64), maximum=array(1, dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "print(\"Observation Spec:\", train_env.observation_spec())\n",
    "print(\"Action Spec:\", train_env.action_spec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fda599d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Episode 1, Reward: [10.]\n",
      "Test Episode 2, Reward: [16.]\n",
      "Test Episode 3, Reward: [15.]\n"
     ]
    }
   ],
   "source": [
    "num_test_episodes = 3\n",
    "for episode in range(num_test_episodes):\n",
    "    time_step = eval_env.reset()\n",
    "    episode_reward = 0\n",
    "    while not time_step.is_last():\n",
    "        action = np.random.choice([0, 1])  \n",
    "        time_step = eval_env.step(action)\n",
    "        episode_reward += time_step.reward\n",
    "    print(f\"Test Episode {episode + 1}, Reward: {episode_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bb0593",
   "metadata": {},
   "source": [
    "# Exercise 2 Create the Network and the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9354a069",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_layer_params = (100, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19ec52f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_net = q_network.QNetwork(\n",
    "    train_env.observation_spec(),\n",
    "    train_env.action_spec(),\n",
    "    fc_layer_params=fc_layer_params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48d710d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "train_step_counter = tf.Variable(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023a962a",
   "metadata": {},
   "source": [
    "# Exercise 3: Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "569c5dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = dqn_agent.DqnAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=optimizer,\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "    train_step_counter=train_step_counter,\n",
    "    epsilon_greedy=0.1, \n",
    "    target_update_period=100,  \n",
    "    gamma=0.99 \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "021ae5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "884672fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=() dtype=int32, numpy=0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.train = common.function(agent.train)\n",
    "agent.train_step_counter.assign(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "533e878b",
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer_max_length = 100000\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=train_env.batch_size,\n",
    "    max_length=replay_buffer_max_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19bb6b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_policy = agent.collect_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fbd1e5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(), train_env.action_spec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3786172f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_step(environment, policy, buffer):\n",
    "    time_step = environment.current_time_step()\n",
    "    action_step = policy.action(time_step)\n",
    "    next_time_step = environment.step(action_step.action)\n",
    "    traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "    buffer.add_batch(traj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ebebe88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_collect_steps = 1000\n",
    "for _ in range(initial_collect_steps):\n",
    "    collect_step(train_env, random_policy, replay_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6114c8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 20000\n",
    "collect_steps_per_iteration = 1\n",
    "batch_size = 64\n",
    "log_interval = 200\n",
    "eval_interval = 1000\n",
    "num_eval_episodes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "19f9bee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Abdel\\anaconda3\\envs\\rl_env\\lib\\site-packages\\tf_agents\\replay_buffers\\tf_uniform_replay_buffer.py:342: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.counter(...)` instead.\n",
      "WARNING:tensorflow:From c:\\Users\\Abdel\\anaconda3\\envs\\rl_env\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:377: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=False) instead.\n"
     ]
    }
   ],
   "source": [
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3,\n",
    "    sample_batch_size=batch_size,\n",
    "    num_steps=2\n",
    ").prefetch(3)\n",
    "iterator = iter(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "50d0aaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_return = 0\n",
    "returns = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "90509358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Abdel\\anaconda3\\envs\\rl_env\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176: calling foldr_v2 (from tensorflow.python.ops.functional_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.foldr(fn, elems, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\n",
      "Step 200, Loss: 0.06393811106681824\n",
      "Step 400, Loss: 0.4865455627441406\n",
      "Step 600, Loss: 0.28280216455459595\n",
      "Step 800, Loss: 0.18510796129703522\n",
      "Step 1000, Loss: 0.29275640845298767\n",
      "Step 1000, Average Return: [145.]\n",
      "Step 1200, Loss: 1.2382336854934692\n",
      "Step 1400, Loss: 0.4353741407394409\n",
      "Step 1600, Loss: 0.42893850803375244\n",
      "Step 1800, Loss: 2.8919677734375\n",
      "Step 2000, Loss: 0.9182277917861938\n",
      "Step 2000, Average Return: [163.]\n",
      "Step 2200, Loss: 1.0051295757293701\n",
      "Step 2400, Loss: 0.8649256229400635\n",
      "Step 2600, Loss: 1.0985932350158691\n",
      "Step 2800, Loss: 2.6630988121032715\n",
      "Step 3000, Loss: 7.576536178588867\n",
      "Step 3000, Average Return: [196.5]\n",
      "Step 3200, Loss: 11.505270957946777\n",
      "Step 3400, Loss: 1.1949626207351685\n",
      "Step 3600, Loss: 1.1867820024490356\n",
      "Step 3800, Loss: 0.8800401091575623\n",
      "Step 4000, Loss: 2.5175998210906982\n",
      "Step 4000, Average Return: [164.6]\n",
      "Step 4200, Loss: 27.4561710357666\n",
      "Step 4400, Loss: 1.7097406387329102\n",
      "Step 4600, Loss: 1.2911977767944336\n",
      "Step 4800, Loss: 1.1261837482452393\n",
      "Step 5000, Loss: 1.2024062871932983\n",
      "Step 5000, Average Return: [191.]\n",
      "Step 5200, Loss: 1.8460946083068848\n",
      "Step 5400, Loss: 1.9972314834594727\n",
      "Step 5600, Loss: 1.229318618774414\n",
      "Step 5800, Loss: 1.855543851852417\n",
      "Step 6000, Loss: 2.1665852069854736\n",
      "Step 6000, Average Return: [198.7]\n",
      "Step 6200, Loss: 1.7393983602523804\n",
      "Step 6400, Loss: 1.6943325996398926\n",
      "Step 6600, Loss: 2.6113622188568115\n",
      "Step 6800, Loss: 3.190136432647705\n",
      "Step 7000, Loss: 17.27582550048828\n",
      "Step 7000, Average Return: [191.6]\n",
      "Step 7200, Loss: 1.7154771089553833\n",
      "Step 7400, Loss: 4.910170078277588\n",
      "Step 7600, Loss: 7.021506309509277\n",
      "Step 7800, Loss: 1.9228343963623047\n",
      "Step 8000, Loss: 1.8927457332611084\n",
      "Step 8000, Average Return: [194.6]\n",
      "Step 8200, Loss: 0.9202723503112793\n",
      "Step 8400, Loss: 62.83858871459961\n",
      "Step 8600, Loss: 5.778400421142578\n",
      "Step 8800, Loss: 7.882761478424072\n",
      "Step 9000, Loss: 0.8312689065933228\n",
      "Step 9000, Average Return: [197.5]\n",
      "Step 9200, Loss: 5.925183296203613\n",
      "Step 9400, Loss: 1.3665680885314941\n",
      "Step 9600, Loss: 3.6705970764160156\n",
      "Step 9800, Loss: 3.1761837005615234\n",
      "Step 10000, Loss: 3.353236675262451\n",
      "Step 10000, Average Return: [190.8]\n",
      "Step 10200, Loss: 1.3490945100784302\n",
      "Step 10400, Loss: 18.29509162902832\n",
      "Step 10600, Loss: 1.5244547128677368\n",
      "Step 10800, Loss: 1.592632532119751\n",
      "Step 11000, Loss: 1.4569021463394165\n",
      "Step 11000, Average Return: [196.]\n",
      "Step 11200, Loss: 4.498955726623535\n",
      "Step 11400, Loss: 1.754568338394165\n",
      "Step 11600, Loss: 5.955375671386719\n",
      "Step 11800, Loss: 1.6640639305114746\n",
      "Step 12000, Loss: 2.6112020015716553\n",
      "Step 12000, Average Return: [178.4]\n",
      "Step 12200, Loss: 1.712817907333374\n",
      "Step 12400, Loss: 2.634737014770508\n",
      "Step 12600, Loss: 3.572282314300537\n",
      "Step 12800, Loss: 25.483322143554688\n",
      "Step 13000, Loss: 3.305673360824585\n",
      "Step 13000, Average Return: [194.1]\n",
      "Step 13200, Loss: 0.9654605984687805\n",
      "Step 13400, Loss: 1.7490334510803223\n",
      "Step 13600, Loss: 5.064432144165039\n",
      "Step 13800, Loss: 24.75482940673828\n",
      "Step 14000, Loss: 1.636366844177246\n",
      "Step 14000, Average Return: [196.8]\n",
      "Step 14200, Loss: 2.9900765419006348\n",
      "Step 14400, Loss: 1.0493210554122925\n",
      "Step 14600, Loss: 2.0522971153259277\n",
      "Step 14800, Loss: 2.9829678535461426\n",
      "Step 15000, Loss: 1.9761409759521484\n",
      "Step 15000, Average Return: [196.6]\n",
      "Step 15200, Loss: 1.2285654544830322\n",
      "Step 15400, Loss: 0.9170247316360474\n",
      "Step 15600, Loss: 43.856388092041016\n",
      "Step 15800, Loss: 1.4353065490722656\n",
      "Step 16000, Loss: 3.1230974197387695\n",
      "Step 16000, Average Return: [195.6]\n",
      "Step 16200, Loss: 3.385772466659546\n",
      "Step 16400, Loss: 1.3773857355117798\n",
      "Step 16600, Loss: 2.1201579570770264\n",
      "Step 16800, Loss: 0.7179120779037476\n",
      "Step 17000, Loss: 0.9644889831542969\n",
      "Step 17000, Average Return: [186.2]\n",
      "Step 17200, Loss: 1.789703369140625\n",
      "Step 17400, Loss: 1.0300042629241943\n",
      "Step 17600, Loss: 20.81934356689453\n",
      "Step 17800, Loss: 2.177623987197876\n",
      "Step 18000, Loss: 24.301359176635742\n",
      "Step 18000, Average Return: [175.1]\n",
      "Step 18200, Loss: 1.276737928390503\n",
      "Step 18400, Loss: 4.695638656616211\n",
      "Step 18600, Loss: 38.84037780761719\n",
      "Step 18800, Loss: 0.8734015226364136\n",
      "Step 19000, Loss: 0.6762319803237915\n",
      "Step 19000, Average Return: [199.5]\n",
      "Step 19200, Loss: 1.116558313369751\n",
      "Step 19400, Loss: 1.054044485092163\n",
      "Step 19600, Loss: 1.2237342596054077\n",
      "Step 19800, Loss: 1.5142900943756104\n",
      "Step 20000, Loss: 5.484848499298096\n",
      "Step 20000, Average Return: [200.]\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(num_iterations):\n",
    "    for _ in range(collect_steps_per_iteration):\n",
    "        collect_step(train_env, collect_policy, replay_buffer)\n",
    "\n",
    "    experience, unused_info = next(iterator)\n",
    "    train_loss = agent.train(experience).loss\n",
    "\n",
    "    step = agent.train_step_counter.numpy()\n",
    "\n",
    "    if step % log_interval == 0:\n",
    "        print(f\"Step {step}, Loss: {train_loss}\")\n",
    "\n",
    "    if step % eval_interval == 0:\n",
    "        total_return = 0\n",
    "        for _ in range(num_eval_episodes):\n",
    "            time_step = eval_env.reset()\n",
    "            episode_return = 0\n",
    "            while not time_step.is_last():\n",
    "                action_step = agent.policy.action(time_step)\n",
    "                time_step = eval_env.step(action_step.action)\n",
    "                episode_return += time_step.reward\n",
    "            total_return += episode_return\n",
    "        avg_return = total_return / num_eval_episodes\n",
    "        print(f\"Step {step}, Average Return: {avg_return}\")\n",
    "        returns.append(avg_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "41f2a29b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Average Return after training: [200.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"Final Average Return after training: {avg_return}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed480f0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
