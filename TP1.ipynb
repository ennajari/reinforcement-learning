{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation des bibliothèques\n",
    "import gymnasium as gym\n",
    "import time\n",
    "import numpy as np\n",
    "import pygame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice 1 : Découverte et Exploration d'un Environnement Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création de l'environnement CartPole\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "observation, info = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Espace d'actions : Discrete(2)\n",
      "Espace d'observations : Box([-4.8               -inf -0.41887903        -inf], [4.8               inf 0.41887903        inf], (4,), float32)\n"
     ]
    }
   ],
   "source": [
    "# Affichage de l'espace d'actions et de l'espace d'observations\n",
    "print(f\"Espace d'actions : {env.action_space}\")\n",
    "print(f\"Espace d'observations : {env.observation_space}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action : 1, Observation : [-0.00881132  0.18421501 -0.0328466  -0.27480096], Reward : 1.0\n",
      "Action : 1, Observation : [-0.00512702  0.37978983 -0.03834261 -0.57766014], Reward : 1.0\n",
      "Action : 0, Observation : [ 0.00246878  0.18522567 -0.04989582 -0.29729834], Reward : 1.0\n",
      "Action : 0, Observation : [ 0.00617329 -0.00915081 -0.05584178 -0.02075977], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.00599027  0.18672566 -0.05625698 -0.33052546], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.00972479  0.3826014  -0.06286748 -0.6404051 ], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.01737682  0.57854086 -0.07567559 -0.9522045 ], Reward : 1.0\n",
      "Action : 0, Observation : [ 0.02894763  0.38451436 -0.09471968 -0.6842251 ], Reward : 1.0\n",
      "Action : 0, Observation : [ 0.03663792  0.19082628 -0.10840418 -0.4228013 ], Reward : 1.0\n",
      "Action : 0, Observation : [ 0.04045444 -0.00260634 -0.11686021 -0.16616398], Reward : 1.0\n",
      "Action : 0, Observation : [ 0.04040232 -0.19587845 -0.12018349  0.08748815], Reward : 1.0\n",
      "Action : 0, Observation : [ 0.03648475 -0.389091   -0.11843372  0.3399675 ], Reward : 1.0\n",
      "Action : 0, Observation : [ 0.02870293 -0.5823462  -0.11163437  0.5930814 ], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.01705601 -0.38585317 -0.09977274  0.26742417], Reward : 1.0\n",
      "Action : 0, Observation : [ 0.00933894 -0.57942015 -0.09442426  0.52704614], Reward : 1.0\n",
      "Action : 1, Observation : [-0.00224946 -0.38310528 -0.08388334  0.20616612], Reward : 1.0\n",
      "Action : 0, Observation : [-0.00991157 -0.5769338  -0.07976002  0.47125375], Reward : 1.0\n",
      "Action : 1, Observation : [-0.02145024 -0.38078117 -0.07033494  0.15453567], Reward : 1.0\n",
      "Action : 0, Observation : [-0.02906587 -0.5748293  -0.06724423  0.4242267 ], Reward : 1.0\n",
      "Action : 1, Observation : [-0.04056245 -0.3788224  -0.0587597   0.11112594], Reward : 1.0\n",
      "Action : 0, Observation : [-0.0481389  -0.57305527 -0.05653718  0.38470703], Reward : 1.0\n",
      "Action : 1, Observation : [-0.0596     -0.37717813 -0.04884304  0.07474772], Reward : 1.0\n",
      "Action : 1, Observation : [-0.06714357 -0.18139124 -0.04734808 -0.23293653], Reward : 1.0\n",
      "Action : 1, Observation : [-0.07077139  0.01437415 -0.05200681 -0.54017067], Reward : 1.0\n",
      "Action : 1, Observation : [-0.07048391  0.21018709 -0.06281023 -0.84877604], Reward : 1.0\n",
      "Action : 1, Observation : [-0.06628016  0.40610686 -0.07978575 -1.1605303 ], Reward : 1.0\n",
      "Action : 1, Observation : [-0.05815803  0.6021724  -0.10299635 -1.477125  ], Reward : 1.0\n",
      "Action : 0, Observation : [-0.04611458  0.40844807 -0.13253886 -1.2183061 ], Reward : 1.0\n",
      "Action : 1, Observation : [-0.03794562  0.6050061  -0.15690498 -1.5494096 ], Reward : 1.0\n",
      "Action : 0, Observation : [-0.0258455   0.41207638 -0.18789317 -1.3095111 ], Reward : 1.0\n",
      "Action : 1, Observation : [-0.01760397  0.60901475 -0.21408339 -1.6546355 ], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.00776491  0.19557433 -0.03780614 -0.3317696 ], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.0116764   0.39121345 -0.04444153 -0.6361309 ], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.01950066  0.5869261  -0.05716415 -0.94247156], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.03123919  0.7827698  -0.07601358 -1.2525544 ], Reward : 1.0\n",
      "Action : 0, Observation : [ 0.04689458  0.58869946 -0.10106467 -0.984616  ], Reward : 1.0\n",
      "Action : 0, Observation : [ 0.05866857  0.39506587 -0.12075699 -0.7253101 ], Reward : 1.0\n",
      "Action : 0, Observation : [ 0.06656989  0.20180234 -0.13526319 -0.47294226], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.07060593  0.39854938 -0.14472203 -0.80501336], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.07857692  0.5953274  -0.1608223  -1.1394945 ], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.09048347  0.79214406 -0.1836122  -1.4779882 ], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.10632635  0.98897123 -0.21317196 -1.8219453 ], Reward : 1.0\n",
      "Action : 1, Observation : [-0.00109987  0.18824026 -0.01024112 -0.3108613 ], Reward : 1.0\n",
      "Action : 0, Observation : [ 0.00266493 -0.00673429 -0.01645835 -0.02142562], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.00253025  0.18861978 -0.01688686 -0.31925556], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.00630264  0.3839781  -0.02327197 -0.6172158 ], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.01398221  0.5794173  -0.03561629 -0.91713655], Reward : 1.0\n",
      "Action : 0, Observation : [ 0.02557055  0.38479453 -0.05395902 -0.6358565 ], Reward : 1.0\n",
      "Action : 0, Observation : [ 0.03326644  0.19046502 -0.06667615 -0.36064273], Reward : 1.0\n",
      "Action : 0, Observation : [ 0.03707574 -0.00364893 -0.073889   -0.08970707], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.03700276  0.19245008 -0.07568314 -0.40475658], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.04085176  0.38855916 -0.08377827 -0.72030777], Reward : 1.0\n",
      "Action : 0, Observation : [ 0.04862295  0.19469008 -0.09818443 -0.45512608], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.05251675  0.39105323 -0.10728695 -0.7770721 ], Reward : 1.0\n",
      "Action : 0, Observation : [ 0.06033782  0.19755742 -0.12282839 -0.51997906], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.06428897  0.39417496 -0.13322797 -0.8487042 ], Reward : 1.0\n",
      "Action : 0, Observation : [ 0.07217246  0.2010971  -0.15020205 -0.60070723], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.07619441  0.3979657  -0.1622162  -0.93667877], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.08415372  0.5948594  -0.18094978 -1.2756248 ], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.09605091  0.79176784 -0.20646228 -1.6190745 ], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.11188626  0.9886377  -0.23884377 -1.9683772 ], Reward : 1.0\n",
      "Action : 0, Observation : [ 0.02311242 -0.21814181  0.04753378  0.33983743], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.01874958 -0.0237273   0.05433052  0.06251473], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.01827504  0.17057529  0.05558082 -0.21254377], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.02168654  0.36486036  0.05132994 -0.487189  ], Reward : 1.0\n",
      "Action : 0, Observation : [ 0.02898375  0.16905314  0.04158616 -0.17878065], Reward : 1.0\n",
      "Action : 0, Observation : [ 0.03236481 -0.02663849  0.03801055  0.12672581], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.03183204  0.1679189   0.04054507 -0.15372705], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.03519042  0.36243758  0.03747053 -0.43334827], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.04243917  0.5570095   0.02880356 -0.7139876 ], Reward : 1.0\n",
      "Action : 0, Observation : [ 0.05357936  0.3615009   0.01452381 -0.4123792 ], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.06080938  0.556414    0.00627622 -0.70044816], Reward : 1.0\n",
      "Action : 0, Observation : [ 0.07193766  0.3612056  -0.00773274 -0.40579614], Reward : 1.0\n",
      "Action : 0, Observation : [ 0.07916177  0.16619416 -0.01584866 -0.11556113], Reward : 1.0\n",
      "Action : 0, Observation : [ 0.08248565 -0.02869717 -0.01815988  0.17207983], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.08191171  0.16667992 -0.01471829 -0.12627621], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.08524531  0.3620096  -0.01724381 -0.423566  ], Reward : 1.0\n",
      "Action : 0, Observation : [ 0.0924855   0.16713612 -0.02571513 -0.13636868], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.09582822  0.36261678 -0.02844251 -0.43705213], Reward : 1.0\n",
      "Action : 0, Observation : [ 0.10308056  0.16790874 -0.03718355 -0.15346918], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.10643873  0.36354282 -0.04025293 -0.4576469 ], Reward : 1.0\n",
      "Action : 0, Observation : [ 0.11370959  0.16901237 -0.04940587 -0.17791875], Reward : 1.0\n",
      "Action : 0, Observation : [ 0.11708984 -0.02536902 -0.05296424  0.09877814], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.11658246  0.17047043 -0.05098868 -0.21013333], Reward : 1.0\n",
      "Action : 0, Observation : [ 0.11999187 -0.02388678 -0.05519135  0.06603944], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.11951413  0.17198125 -0.05387056 -0.24353325], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.12295376  0.36782962 -0.05874122 -0.55270994], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.13031034  0.5637252  -0.06979542 -0.86330646], Reward : 1.0\n",
      "Action : 0, Observation : [ 0.14158484  0.3696193  -0.08706155 -0.5933597 ], Reward : 1.0\n",
      "Action : 0, Observation : [ 0.14897723  0.17581698 -0.09892875 -0.32932106], Reward : 1.0\n",
      "Action : 0, Observation : [ 0.15249358 -0.01776769 -0.10551517 -0.06940147], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.15213822  0.17869635 -0.1069032  -0.39342248], Reward : 1.0\n",
      "Action : 0, Observation : [ 0.15571214 -0.01475907 -0.11477165 -0.13626564], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.15541697  0.18180376 -0.11749696 -0.46283892], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.15905304  0.37837318 -0.12675373 -0.7901248 ], Reward : 1.0\n",
      "Action : 0, Observation : [ 0.16662051  0.18519853 -0.14255624 -0.5398533 ], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.17032447  0.3820063  -0.1533533  -0.87384003], Reward : 1.0\n",
      "Action : 0, Observation : [ 0.1779646   0.18926461 -0.1708301  -0.63302976], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.1817499  0.386306  -0.1834907 -0.9742698], Reward : 1.0\n",
      "Action : 0, Observation : [ 0.18947601  0.19405626 -0.2029761  -0.74438024], Reward : 1.0\n"
     ]
    }
   ],
   "source": [
    "iteration = 0\n",
    "while iteration < 100:\n",
    "    for event in pygame.event.get():\n",
    "        if event.type == pygame.QUIT:\n",
    "            env.close()\n",
    "            break\n",
    "    \n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    print(f\"Action : {action}, Observation : {observation}, Reward : {reward}\")\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "    \n",
    "    iteration += 1\n",
    "    time.sleep(0.01)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice 2 : Manipulation des Observations et Récompenses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "observation, info = env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = env.action_space.sample()\n",
    "observation, reward, terminated, truncated, info = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation: [ 0.01865603  0.14722326 -0.00694064 -0.29809844]\n",
      "Récompense: 1.0\n",
      "Épisode terminé: False\n",
      "Épisode tronqué: False\n",
      "Informations supplémentaires: {}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Observation: {observation}\")\n",
    "print(f\"Récompense: {reward}\")\n",
    "print(f\"Épisode terminé: {terminated}\")\n",
    "print(f\"Épisode tronqué: {truncated}\")\n",
    "print(f\"Informations supplémentaires: {info}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pas 1 - Action: 0, Observation: [ 0.0216005  -0.04779907 -0.01290261 -0.00761254], Reward: 1.0\n",
      "Pas 2 - Action: 1, Observation: [ 0.02064452  0.14750552 -0.01305486 -0.3043383 ], Reward: 1.0\n",
      "Pas 3 - Action: 1, Observation: [ 0.02359463  0.34281108 -0.01914163 -0.6011097 ], Reward: 1.0\n",
      "Pas 4 - Action: 1, Observation: [ 0.03045085  0.5381955  -0.03116382 -0.8997599 ], Reward: 1.0\n",
      "Pas 5 - Action: 1, Observation: [ 0.04121476  0.7337256  -0.04915902 -1.2020733 ], Reward: 1.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    for event in pygame.event.get():\n",
    "        if event.type == pygame.QUIT:\n",
    "            env.close()\n",
    "            break\n",
    "    \n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    print(f\"Pas {i+1} - Action: {action}, Observation: {observation}, Reward: {reward}\")\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "        print(\"Environnement réinitialisé\")\n",
    "    \n",
    "    time.sleep(0.1)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice 3 : Contrôle Manuel de l'Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "observation, info = env.reset()\n",
    "total_reward = 0\n",
    "steps = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running = True\n",
    "while running:\n",
    "    for event in pygame.event.get():\n",
    "        if event.type == pygame.QUIT:\n",
    "            env.close()\n",
    "            break\n",
    "    \n",
    "    user_input = input(\"Entrez une action (0 ou 1, q pour quitter): \")\n",
    "    if user_input.lower() == 'q':\n",
    "        running = False\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        action = int(user_input)\n",
    "        if action not in [0, 1]:\n",
    "            print(\"Action invalide, doit être 0 ou 1\")\n",
    "            continue\n",
    "    except ValueError:\n",
    "        print(\"Entrée invalide, veuillez entrer 0, 1 ou q\")\n",
    "        continue\n",
    "    \n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    steps += 1\n",
    "    total_reward += reward\n",
    "    \n",
    "    print(f\"Position: {observation[0]}, Angle: {observation[2]}, Reward: {reward}, Total: {total_reward}\")\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        print(f\"Épisode terminé après {steps} pas avec une récompense totale de {total_reward}\")\n",
    "        observation, info = env.reset()\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        print(\"Environnement réinitialisé\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice 4 : Évaluation des Performances d'une Politique Aléatoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "num_episodes = 10\n",
    "episode_durations = []\n",
    "episode_steps = []\n",
    "\n",
    "print(f\"Exécution de {num_episodes} épisodes avec des actions aléatoires\")\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    observation, info = env.reset()\n",
    "    steps = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    while True:\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                env.close()\n",
    "                break\n",
    "        \n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        steps += 1\n",
    "        \n",
    "        if terminated or truncated:\n",
    "            duration = time.time() - start_time\n",
    "            episode_durations.append(duration)\n",
    "            episode_steps.append(steps)\n",
    "            print(f\"Épisode {episode+1}: {steps} pas, durée: {duration:.2f} secondes\")\n",
    "            break\n",
    "        \n",
    "        time.sleep(0.01)\n",
    "\n",
    "if episode_durations:\n",
    "    average_duration = np.mean(episode_durations)\n",
    "    average_steps = np.mean(episode_steps)\n",
    "    print(f\"\\nDurée moyenne des épisodes: {average_duration:.2f} secondes\")\n",
    "    print(f\"Nombre de pas moyen: {average_steps:.2f}\")\n",
    "    print(f\"Durée minimale: {np.min(episode_durations):.2f} secondes\")\n",
    "    print(f\"Durée maximale: {np.max(episode_durations):.2f} secondes\")\n",
    "    print(f\"Écart-type des durées: {np.std(episode_durations):.2f} secondes\")\n",
    "else:\n",
    "    print(\"Aucune donnée collectée pour l'analyse.\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
