{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercice 1 : Exploration de l'Environnement FrozenLake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Espace d'états: Discrete(16)\n",
      "Espace d'actions: Discrete(4)\n"
     ]
    }
   ],
   "source": [
    "# Charger l’environnement FrozenLake\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=True, render_mode=\"ansi\")\n",
    "print(\"Espace d'états:\", env.observation_space)\n",
    "print(\"Espace d'actions:\", env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Épisode 1, État: 0, Action: 0, Récompense: 0.0\n",
      "Épisode 1, État: 0, Action: 2, Récompense: 0.0\n",
      "Épisode 1, État: 4, Action: 2, Récompense: 0.0\n",
      "Épisode 1, État: 0, Action: 1, Récompense: 0.0\n",
      "Épisode 1, État: 1, Action: 0, Récompense: 0.0\n",
      "Épisode 1, État: 0, Action: 3, Récompense: 0.0\n",
      "Épisode 1, État: 0, Action: 3, Récompense: 0.0\n",
      "Épisode 1, État: 0, Action: 2, Récompense: 0.0\n",
      "Épisode 1, État: 0, Action: 0, Récompense: 0.0\n",
      "Épisode 1, État: 4, Action: 3, Récompense: 0.0\n",
      "Épisode 2, État: 0, Action: 1, Récompense: 0.0\n",
      "Épisode 2, État: 0, Action: 0, Récompense: 0.0\n",
      "Épisode 2, État: 0, Action: 3, Récompense: 0.0\n",
      "Épisode 2, État: 0, Action: 0, Récompense: 0.0\n",
      "Épisode 2, État: 0, Action: 3, Récompense: 0.0\n",
      "Épisode 2, État: 1, Action: 2, Récompense: 0.0\n",
      "Épisode 2, État: 1, Action: 3, Récompense: 0.0\n",
      "Épisode 2, État: 2, Action: 1, Récompense: 0.0\n",
      "Épisode 2, État: 6, Action: 1, Récompense: 0.0\n",
      "Épisode 3, État: 0, Action: 2, Récompense: 0.0\n",
      "Épisode 3, État: 0, Action: 3, Récompense: 0.0\n",
      "Épisode 3, État: 0, Action: 2, Récompense: 0.0\n",
      "Épisode 3, État: 0, Action: 1, Récompense: 0.0\n",
      "Épisode 3, État: 4, Action: 0, Récompense: 0.0\n",
      "Épisode 3, État: 4, Action: 0, Récompense: 0.0\n",
      "Épisode 3, État: 4, Action: 0, Récompense: 0.0\n",
      "Épisode 3, État: 0, Action: 0, Récompense: 0.0\n",
      "Épisode 3, État: 0, Action: 2, Récompense: 0.0\n",
      "Épisode 3, État: 4, Action: 0, Récompense: 0.0\n",
      "Épisode 3, État: 0, Action: 2, Récompense: 0.0\n",
      "Épisode 3, État: 4, Action: 0, Récompense: 0.0\n",
      "Épisode 3, État: 8, Action: 0, Récompense: 0.0\n",
      "Épisode 3, État: 4, Action: 3, Récompense: 0.0\n",
      "Épisode 4, État: 0, Action: 1, Récompense: 0.0\n",
      "Épisode 4, État: 1, Action: 2, Récompense: 0.0\n",
      "Épisode 4, État: 1, Action: 0, Récompense: 0.0\n",
      "Épisode 4, État: 0, Action: 1, Récompense: 0.0\n",
      "Épisode 4, État: 1, Action: 0, Récompense: 0.0\n",
      "Épisode 4, État: 0, Action: 2, Récompense: 0.0\n",
      "Épisode 4, État: 4, Action: 3, Récompense: 0.0\n",
      "Épisode 4, État: 0, Action: 1, Récompense: 0.0\n",
      "Épisode 4, État: 1, Action: 0, Récompense: 0.0\n",
      "Épisode 4, État: 0, Action: 1, Récompense: 0.0\n",
      "Épisode 4, État: 4, Action: 2, Récompense: 0.0\n",
      "Épisode 5, État: 0, Action: 3, Récompense: 0.0\n",
      "Épisode 5, État: 0, Action: 0, Récompense: 0.0\n",
      "Épisode 5, État: 4, Action: 1, Récompense: 0.0\n",
      "Épisode 5, État: 8, Action: 3, Récompense: 0.0\n",
      "Épisode 5, État: 8, Action: 2, Récompense: 0.0\n",
      "Épisode 5, État: 4, Action: 0, Récompense: 0.0\n",
      "Épisode 5, État: 8, Action: 0, Récompense: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Exécution d'une boucle d'interaction aléatoire\n",
    "num_episodes = 5\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()[0]\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = env.action_space.sample()  # Choix aléatoire d'une action\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        print(f\"Épisode {episode+1}, État: {state}, Action: {action}, Récompense: {reward}\")\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercice 2 : Implémentation de la Q-Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q-Table initiale :\n",
      " [[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "num_states = env.observation_space.n\n",
    "num_actions = env.action_space.n\n",
    "q_table = np.zeros((num_states, num_actions))\n",
    "print(\"\\nQ-Table initiale :\\n\", q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercice 3 : Implémentation du Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1 \n",
    "gamma = 0.99  # Facteur de discount\n",
    "epsilon = 1.0  # Probabilité d'exploration\n",
    "epsilon_decay = 0.99  # Réduction progressive d'epsilon\n",
    "epsilon_min = 0.01  # Valeur minimale d'epsilon\n",
    "num_episodes = 5000  # Nombre d'épisodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q-Table après apprentissage :\n",
      " [[4.51261598e-01 3.27202716e-01 4.24400238e-01 3.83616041e-01]\n",
      " [8.39270957e-02 2.06148740e-01 2.00289590e-02 4.12888761e-05]\n",
      " [2.16609470e-01 4.54506467e-02 2.81647712e-02 8.05281596e-02]\n",
      " [1.51059811e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [4.68230800e-01 2.51245700e-01 3.07670628e-01 2.99511504e-01]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [2.35648906e-01 4.44087281e-02 4.60382300e-02 3.89128110e-02]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [3.48854972e-01 3.17050232e-01 3.91892715e-01 5.08697830e-01]\n",
      " [4.89743938e-01 5.70186903e-01 4.37221809e-01 4.42919484e-01]\n",
      " [6.17890870e-01 2.40981700e-01 2.77602649e-01 1.45347395e-01]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [3.37557365e-01 5.37641232e-01 6.49707659e-01 4.04055989e-01]\n",
      " [4.21085679e-01 8.54130347e-01 7.11376918e-01 6.25707656e-01]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "for episode in range(num_episodes):\n",
    "    state = env.reset()[0]\n",
    "    done = False\n",
    "    while not done:\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = env.action_space.sample() \n",
    "        else:\n",
    "            action = np.argmax(q_table[state]) \n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        q_table[state, action] = q_table[state, action] + alpha * (reward + gamma * np.max(q_table[next_state]) - q_table[state, action])\n",
    "        state = next_state\n",
    "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "print(\"\\nQ-Table après apprentissage :\\n\", q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercice 4 : Évaluation des Performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Taux de réussite après apprentissage : 76/100 (76.00%)\n"
     ]
    }
   ],
   "source": [
    "num_test_episodes = 100\n",
    "successes = 0\n",
    "\n",
    "for _ in range(num_test_episodes):\n",
    "    state = env.reset()[0]\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = np.argmax(q_table[state])  \n",
    "        state, reward, done, _, _ = env.step(action)\n",
    "        if reward == 1:\n",
    "            successes += 1\n",
    "print(f\"\\nTaux de réussite après apprentissage : {successes}/{num_test_episodes} ({(successes/num_test_episodes)*100:.2f}%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
